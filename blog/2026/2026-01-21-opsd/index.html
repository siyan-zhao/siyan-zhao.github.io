<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="the-challenge-of-current-training-paradigms">The Challenge of Current Training Paradigms</h2> <p>Large language models have shown impressive abilities in complex reasoning tasks, but finding more efficient and effective ways to train them remains an active area of research. Current approaches come with their own trade-offs:</p> <p><strong>Supervised Fine-Tuning (SFT)</strong> uses expert demonstrations for training but might encounter exposure bias—the model’s training distribution diverges from its test-time behavior, as it never sees its own errors during training. This distributional mismatch can lead to compounding errors during autoregressive generation.</p> <p><strong>Reinforcement Learning (RL)</strong> methods, such as Group Relative Policy Optimization (GRPO), has better generalization through on-policy training. It suffers from computational inefficiency, requiring multiple rollouts per problem instance (typically 8 or more), and provide only sparse, sequence-level feedback signals. The binary nature of outcome verification—correct or incorrect—offers no intermediate guidance regarding which specific reasoning steps are suboptimal.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/opsd_tab1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Comparison of training methods for reasoning tasks. On-Policy Self-Distillation (OPSD) combines the advantages of on-policy training with dense feedback without requiring an external teacher model. </div> <h2 id="core-insight">Core Insight</h2> <p>Our approach draws inspiration from human learning. When students struggle with a problem, rather than relying on extended trial-and-error, they can examine the correct solution, understand the reasoning steps, and identify where their own reasoning went wrong. Prior work has shown that for LLMs, evaluation is often easier than generation. We hypothesize that rationalization—explaining a given correct answer—is similarly easier than generation.</p> <p>Given that modern LLMs already exhibit strong reasoning capabilities, we ask this research question: <strong>Can a model effectively serve as its own teacher through self-distillation?</strong> Specifically, when provided with ground-truth solutions as privileged information, can a sufficiently capable model rationalize the reasoning steps and provide dense token-level supervision to guide its weaker self—the version without access to privileged information?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/opsd_main.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2: Overview of On-Policy Self-Distillation framework. A single language model instantiates both student and teacher policies through differential conditioning contexts. </div> <p>We show that the answer is yes through <strong>On-Policy Self-Distillation (OPSD)</strong>, where a single model plays two roles:</p> <ul> <li> <strong>Student policy</strong> \(p_S(\cdot \mid x)\): observes only the problem \(x\), replicating inference-time conditions</li> <li> <strong>Teacher policy</strong> \(p_T(\cdot \mid x, y^*)\): receives privileged access to the ground-truth solution \(y^*\)</li> </ul> <p>Critically, both policies share same parameters but are under different conditioning contexts.</p> <h2 id="methodology">Methodology</h2> <p>The training procedure consists of three steps:</p> <p><strong>1. On-Policy Generation.</strong> For a given problem \(x\), the student policy samples its own attempted solution:</p> \[\hat{y} \sim p_S(\cdot \mid x)\] <p><strong>2. Dual Evaluation.</strong> Evaluations are done on the student’s generations: At each token position \(n\), both policies generate probability distributions over the next token:</p> <ul> <li>Student: \(p_S(\cdot \mid x, \hat{y}_{&lt;n})\)</li> <li>Teacher: \(p_T(\cdot \mid x, y^*, \hat{y}_{&lt;n})\)</li> </ul> <p>The teacher policy, informed by the correct solution \(y^*\), provides guidance toward reasoning trajectories that lead to the correct answer.</p> <p><strong>3. Distribution Matching.</strong> We train the student to minimize divergence from the teacher’s distribution at every token position:</p> <div class="equation-block"> $$ \mathcal{L}_{\text{OPSD}}(\theta) = \mathbb{E}_{(x,y^*)\sim \mathcal{S}} \mathbb{E}_{\hat{y}\sim p_S(\cdot|x)} \left[ \sum_{n=1}^{|\hat{y}|} D\left(p_T(\cdot \mid x, y^*, \hat{y}_{&lt;n}) \,\|\, p_S(\cdot \mid x, \hat{y}_{&lt;n})\right) \right] $$ </div> <p><strong>Gradients flow only through the student’s logits</strong>. The teacher serves as a fixed supervision target, despite both policies sharing the same underlying parameters but differing in their conditioning contexts.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/prompt_exampel_opsd.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3: Prompt construction for student and teacher policies. Both policies operate with identical parameters but receive different conditioning information. </div> <h2 id="experimental-results">Experimental Results</h2> <p>We evaluate OPSD on competition-level mathematical reasoning benchmarks (AIME 2024/2025, HMMT 2025, AMO-Bench) using the Qwen3 model family (1.7B, 4B, and 8B parameters).</p> <h3 id="performance-comparison">Performance Comparison</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/main_result.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4: Main results comparing OPSD against baseline methods across benchmarks. </div> <h3 id="computational-efficiency">Computational Efficiency</h3> <p>A particularly notable finding concerns computational efficiency. OPSD achieves superior performance while utilizing <strong>8× fewer tokens</strong> than GRPO:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/opsd_tokenefficiency.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5: Token efficiency comparison between OPSD and GRPO. OPSD achieves higher performance with substantially reduced token consumption. </div> <p>This efficiency advantage stems from:</p> <ol> <li>Dense token-level supervision (versus sparse sequence-level rewards)</li> <li>Reduced generation budgets (2k tokens versus 16k for GRPO)</li> </ol> <p>In practice, this translates to <strong>reduced training time</strong> and <strong>lower computational requirements</strong>.</p> <h2 id="ablation-studies">Ablation Studies</h2> <h3 id="model-capacity-requirements">Model Capacity Requirements</h3> <p>Self-distillation requires sufficient model capacity to effectively rationalize solutions. We conducted experiments across three model scales:</p> <div class="highlight-box"> <strong>Key Finding:</strong> A minimum capacity threshold exists for effective self-distillation. </div> <ul> <li> <strong>1.7B parameters</strong>: OPSD yields marginal or negative effects</li> <li> <strong>4B parameters</strong>: Consistent improvements emerge</li> <li> <strong>8B parameters</strong>: Most substantial gains observed</li> </ul> <p>Below the capacity threshold, models lack the requisite reasoning ability to understand and rationalize why correct solutions work, even when provided access to them.</p> <h3 id="generation-length-analysis">Generation Length Analysis</h3> <p>Since the training objective operates at the token level, generation length directly impacts the quantity of supervision signal available:</p> <p>Increasing generation length from 1k → 2k → 4k tokens yields consistent performance improvements. This finding aligns with the intuitive understanding that more tokens provide additional opportunities for the teacher policy to guide the student’s reasoning process.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/gen_length_ablation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 6: Pass@k performance curves showing the effect of generation length. Longer generation budgets provide increased teacher supervision. </div> <h3 id="objective-function-comparison">Objective Function Comparison</h3> <p>We compared two approaches for computing the divergence:</p> <ol> <li> <strong>Full-vocabulary divergence</strong>: Compute \(D(p_T \| p_S)\) over the entire vocabulary</li> <li> <strong>Sampled-token objective</strong>: Utilize only the sampled token in a policy-gradient formulation</li> </ol> <p>Results on Qwen3-4B:</p> <table> <thead> <tr> <th>Method</th> <th>AIME25</th> <th>HMMT25</th> </tr> </thead> <tbody> <tr> <td>Full-vocabulary</td> <td><strong>84.1%</strong></td> <td><strong>60.0%</strong></td> </tr> <tr> <td>Sampled-token</td> <td>82.1%</td> <td>57.3%</td> </tr> </tbody> </table> <p>The full-vocabulary approach provides richer supervision by exposing the student to the teacher’s complete distribution over plausible next tokens, though at increased memory cost.</p> <h2 id="limitations-and-future-directions">Limitations and Future Directions</h2> <p>While OPSD demonstrates strong empirical results up to 8B parameters, several research directions warrant investigation:</p> <p><strong>Scaling to larger models.</strong> The scaling behavior of self-distillation beyond 8B parameters remains an open question. Our hypothesis suggests larger models should exhibit greater benefits, but computational constraints precluded testing at frontier model scales.</p> <p><strong>Verification signal integration.</strong> The current framework does not explicitly incorporate correctness verification. Combining distribution matching with outcome-based verification signals could provide complementary learning objectives.</p> <p><strong>Curriculum learning strategies.</strong> When problems exceed the model’s comprehension threshold, even the teacher policy cannot provide meaningful supervision. Adaptive difficulty adjustment—gradually increasing problem complexity as the model improves—could enhance training effectiveness.</p> <h2 id="conclusion">Conclusion</h2> <p>On-Policy Self-Distillation demonstrates that sufficiently capable models can provide self-supervision by leveraging their ability to rationalize correct solutions. By conditioning a single model on different contexts—with and without privileged information—OPSD achieves:</p> <ul> <li>Superior performance compared to supervised fine-tuning</li> <li>Comparable or better results than reinforcement learning with <strong>8× improved token efficiency</strong> </li> </ul> <p>The central insight is conceptually elegant yet empirically powerful: evaluation and rationalization are computationally less demanding than generation from scratch. When provided access to correct answers, models can understand the underlying reasoning and effectively guide their weaker counterparts toward improved solutions.</p> <p>As models continue to scale, this self-teaching capability may become increasingly valuable—enabling more efficient training without the overhead of maintaining separate teacher models or the computational burden of extensive reinforcement learning.</p> <hr> <p><strong>Additional resources:</strong> The complete paper includes detailed ablations, implementation specifications, and analysis. All experiments employ the Qwen3 model family with LoRA fine-tuning on 8×A100 GPUs. Code and additional materials are available in the full publication.</p> </body></html>