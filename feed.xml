<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://siyan-zhao.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://siyan-zhao.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-22T07:13:14+00:00</updated><id>https://siyan-zhao.github.io/feed.xml</id><title type="html">Siyan Zhao</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Self-Distilled Reasoner: On-Policy Self-Distillation</title><link href="https://siyan-zhao.github.io/blog/2026/opsd/" rel="alternate" type="text/html" title="Self-Distilled Reasoner: On-Policy Self-Distillation"/><published>2026-01-21T00:00:00+00:00</published><updated>2026-01-21T00:00:00+00:00</updated><id>https://siyan-zhao.github.io/blog/2026/opsd</id><content type="html" xml:base="https://siyan-zhao.github.io/blog/2026/opsd/"><![CDATA[<div class="author-block"> <div class="author-list"> Siyan Zhao<sup>†,1</sup>, Zhihui Xie<sup>2</sup>, Mengchen Liu<sup>3</sup>, Jing Huang<sup>3</sup>, Guan Pang<sup>3</sup>, Feiyu Chen<sup>*,‡,3</sup>, Aditya Grover<sup>*,1</sup> </div> <div class="affiliation-list"> <sup>1</sup>UCLA &nbsp;&nbsp; <sup>2</sup>HKU &nbsp;&nbsp; <sup>3</sup>Meta Superintelligence Labs </div> <div class="author-note"> <sup>*</sup>Equal advising &nbsp;&nbsp; <sup>†</sup>Work done at UCLA and during Siyan's part-time internship at Meta &nbsp;&nbsp; <sup>‡</sup>Work done at Meta </div> </div> <h2 id="the-challenge-of-current-llm-training-paradigms">The Challenge of Current LLM Training Paradigms</h2> <p>LLMs have shown impressive abilities in complex reasoning tasks, but finding more efficient and effective ways to train them remains an active area of research. Current popular approaches to improve reasoning come with their own trade-offs:</p> <p><strong>Supervised Fine-Tuning (SFT)</strong> uses expert demonstrations for training but might encounter exposure bias<d-cite key="chu2025sft"></d-cite>—the model’s training distribution diverges from its test-time behavior, as it never sees its own errors during training. This distributional mismatch can lead to compounding errors during autoregressive generation.</p> <p><strong>Reinforcement Learning (RL)</strong> methods, such as Group Relative Policy Optimization (GRPO)<d-cite key="shao2024deepseekmath"></d-cite><d-cite key="guo2025deepseek"></d-cite><d-cite key="yu2025dapo"></d-cite>, have better generalization through on-policy training. They suffer from computational inefficiency, requiring multiple rollouts per problem instance (typically 8 or more), and often receive only sparse, sequence-level feedback signals. The binary nature of outcome verification—correct or incorrect—offers no intermediate guidance regarding which specific reasoning steps are suboptimal. Moreover, when all samples are either correct or incorrect, the gradient signal vanishes, further limiting the learning signal.</p> <p><strong>Knowledge Distillation</strong> traditionally provides dense token-level supervision from a teacher model but relies on off-policy data<d-cite key="hinton2015distillingknowledgeneuralnetwork"></d-cite>. Recent advances in <strong>on-policy distillation</strong><d-cite key="agarwal2024policy"></d-cite><d-cite key="lu2025onpolicydistillation"></d-cite>—where a student model samples its own trajectories while a teacher policy provides dense token-level supervision—have demonstrated superior sample efficiency by combining the distributional realism of on-policy training with dense feedback.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/opsd_tab1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Comparison of training methods for reasoning tasks. On-Policy Self-Distillation (OPSD) combines the advantages of on-policy training with dense feedback without requiring an external teacher model. </div> <h2 id="core-insight">Core Insight</h2> <p>Our approach draws inspiration from human learning. When students struggle with a problem, rather than relying on extended trial-and-error, they can examine the correct solution, understand the reasoning steps, and identify where their own reasoning went wrong. Prior work has shown that for LLMs, evaluation is often easier than generation<d-cite key="sun2024easy"></d-cite><d-cite key="naor1996evaluation"></d-cite>. We hypothesize that rationalization—explaining a given correct answer—is similarly easier than generation.</p> <p>Given that modern LLMs already exhibit strong reasoning capabilities, we ask this research question: <strong>Can a model effectively serve as its own teacher through self-distillation?</strong> Specifically, when provided with ground-truth solutions as privileged information, can a sufficiently capable model rationalize the reasoning steps and provide dense token-level supervision to guide its weaker self—the version without access to privileged information?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/opsd_main.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: Overview of On-Policy Self-Distillation framework. A single language model instantiates both student and teacher policies through differential conditioning contexts. </div> <p>We show that the answer is yes through <strong>On-Policy Self-Distillation (OPSD)</strong>, where a single model plays two roles:</p> <ul> <li><strong>Student policy</strong> \(p_S(\cdot \mid x)\): observes only the problem \(x\), replicating inference-time conditions</li> <li><strong>Teacher policy</strong> \(p_T(\cdot \mid x, y^*)\): receives privileged access to the ground-truth solution \(y^*\)</li> </ul> <p>Critically, both policies share the same parameters but operate under different conditioning contexts.</p> <h2 id="methodology">Methodology</h2> <p>The training procedure consists of three steps:</p> <p><strong>1. On-Policy Sampling from the Student.</strong> For a given problem \(x\), the student policy samples its own attempted solution:</p> \[\hat{y} = (\hat{y}_1,\ldots,\hat{y}_{|\hat{y}|}) \sim p_S(\cdot \mid x)\] <p><strong>2. Teacher-Student Distribution Computation.</strong> Both policies evaluate the student’s generated trajectory \(\hat{y}\). At each token position \(n\), they compute probability distributions over the next token \(y_n \in \mathcal{V}\) conditioned on the same student prefix \(\hat{y}_{\lt n} = (\hat{y}_1,\ldots,\hat{y}_{n-1})\):</p> \[p_S(y_n \mid x, \hat{y}_{\lt n}), \qquad p_T(y_n \mid x, y^*, \hat{y}_{\lt n})\] <p>The teacher policy, informed by the correct solution \(y^*\), provides guidance toward reasoning trajectories that lead to the correct answer.</p> <p><strong>3. Per-Token Distribution Matching.</strong> We instantiate a <strong>full-vocabulary divergence objective</strong> that matches the teacher and student next-token distributions at each position. We define the trajectory-averaged, token-wise divergence:</p> <div class="equation-block"> $$ D(p_T \| p_S)(\hat{y} \mid x) = \frac{1}{|\hat{y}|} \sum_{n=1}^{|\hat{y}|} D\left(p_T(\cdot \mid x, y^*, \hat{y}_{\lt n}) \,\|\, p_S(\cdot \mid x, \hat{y}_{\lt n})\right) $$ </div> <p>where \(D\) can be any distribution divergence measure such as the generalized Jensen-Shannon divergence \(\text{JSD}_\beta\), defined for a weight \(\beta \in [0, 1]\) as:</p> \[\text{JSD}_\beta(p_T \| p_S) = \beta D_{\text{KL}}(p_T \| m) + (1 - \beta) D_{\text{KL}}(p_S \| m)\] <p>where \(m = \beta p_T + (1 - \beta) p_S\) is the interpolated mixture distribution. This full-vocabulary formulation provides dense, token-level feedback: the teacher, informed by \(y^*\), exposes the student to the entire distribution over plausible next tokens and guides it toward reasoning paths that lead to the correct answer.</p> <p>We minimize the expected divergence between teacher and student over on-policy student samples:</p> <div class="equation-block"> $$ \mathcal{L}(\theta) = \mathbb{E}_{(x,y^*)\sim \mathcal{S}} \left[ \mathbb{E}_{\hat{y}\sim p_S(\cdot|x)} \left[ D(p_T \| p_S)(\hat{y} \mid x) \right] \right] $$ </div> <p><strong>Gradients flow only through the student’s logits</strong>. The teacher serves as a fixed supervision target, despite both policies sharing the same underlying parameters but differing in their conditioning contexts.</p> <p>Importantly, we fix the teacher policy to be the initial policy, rather than the currently updating learning policy, as we find this helps stabilize training and implicitly acts as regularization to prevent excessive deviation from the initial policy.</p> <p><strong>Alternative Objective: Sampled-Token Distillation.</strong> As an alternative approach, we can use a policy-gradient formulation that operates only on sampled tokens. For each position \(n\), we define the advantage term:</p> \[A_n(x, \hat{y}) = \log p_T(\hat{y}_n \mid x, y^*, \hat{y}_{\lt n}) - \log p_S(\hat{y}_n \mid x, \hat{y}_{\lt n})\] <p>and optimize:</p> \[\mathcal{L}(\theta) = - \mathbb{E}_{(x,y^*) \sim \mathcal{S}} \left[ \mathbb{E}_{\hat{y} \sim p_S(\cdot \mid x)} \left[ \frac{1}{|\hat{y}|} \sum_{n=1}^{|\hat{y}|} A_n(x, \hat{y}) \log p_S(\hat{y}_n \mid x, \hat{y}_{\lt n}) \right] \right]\] <p>where \(A_n(x,\hat{y})\) is treated as a constant (gradients do not flow through the advantage). Compared to the full-vocabulary divergence objective, this sampled-token approach uses the teacher’s log-probabilities to provide dense trajectory-level shaping signals without explicitly matching the full distribution at each step.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/prompt_exampel_opsd.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3: Prompt construction for student and teacher policies. Both policies operate with identical parameters but receive different conditioning information. </div> <h2 id="experimental-results">Experimental Results</h2> <p>We evaluate OPSD on competition-level mathematical reasoning benchmarks (AIME 2024/2025, HMMT 2025, AMO-Bench) using the Qwen3 model family (1.7B, 4B, and 8B parameters).</p> <h3 id="performance-comparison">Performance Comparison</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/main_result.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4: Main results comparing OPSD against baseline methods across benchmarks. </div> <h3 id="computational-efficiency">Computational Efficiency</h3> <p>OPSD achieves superior performance while utilizing <strong>4-8× fewer tokens</strong> than GRPO:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/opsd_tokenefficiency.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5: Token efficiency comparison. With equal generations per update but shorter sequences (2k vs 16k tokens), OPSD achieves comparable or better performance while being 4–8× more token-efficient than GRPO. </div> <p>These efficiency and efficacy advantages stem from:</p> <ol> <li>Dense token-level supervision (versus sparse sequence-level rewards)</li> <li>Reduced generation budgets (2k tokens versus 16k for GRPO)</li> </ol> <p>In practice, this translates to <strong>reduced training time</strong> and <strong>lower computational requirements</strong>.</p> <h2 id="discussions">Discussions</h2> <h3 id="self-rationalization-ability-scales-with-model-ability">Self-rationalization ability scales with model ability</h3> <p>OPSD relies on a gap between two conditional views of the same model: the teacher distribution \(p_T(\cdot \mid x, y^\star)\), which has access to the verified solution, and the student distribution \(p_S(\cdot \mid x)\), which does not. For self-distillation to be effective, conditioning on \(y^\star\) must reliably produce a <em>better-informed</em> next-token distribution along the student’s own prefixes \(\hat{y}_{&lt;n}\). This requires the model to be capable of understanding and explaining why a solution is correct, not merely recognizing it.</p> <p>When model capacity is insufficient, access to \(y^\star\) does not consistently sharpen the teacher’s distribution or improve token-level guidance. In this regime, the teacher signal is weak or unstable, so matching \(p_T\) provides little benefit and can even introduce noise. As capacity increases, the teacher becomes better at interpreting the problem through the lens of the correct solution and assigning higher probability to reasoning steps that stay on a valid path.</p> <p>This pattern appears clearly across Qwen3 scales. At 1.7B parameters, OPSD provides only marginal or mixed gains relative to GRPO, indicating that the model is below the capacity threshold needed for effective self-rationalization. At 4B, improvements become consistent, suggesting that the teacher signal is now sufficiently informative. At 8B, gains are largest, supporting the view that larger models can better leverage privileged information to guide their own learning.</p> <h3 id="generation-length-analysis">Generation Length Analysis</h3> <p>Since the training objective operates at the token level, generation length directly impacts the quantity of supervision signal available:</p> <p>Increasing generation length from 1k → 2k → 4k tokens yields consistent performance improvements. This finding aligns with the intuitive understanding that more tokens provide additional opportunities for the teacher policy to guide the student’s reasoning process.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/gen_length_ablation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 6: Pass@k performance curves showing the effect of generation length. Longer generation budgets provide increased teacher supervision. </div> <h3 id="objective-function-comparison">Objective Function Comparison</h3> <p>OPSD can compute the teacher–student discrepancy either as a full-vocabulary divergence \(D(p_T \| p_S)\) at every position or as a sampled-token, policy-gradient-style objective. The key difference is the amount of information passed to the student. Full-vocabulary distillation exposes the student to the teacher’s entire distribution over plausible next tokens, providing fine-grained guidance about which alternatives are more or less compatible with correct reasoning. In contrast, sampled-token objectives only use the probability of the token actually generated.</p> <p>Results on Qwen3-4B (pass@8 accuracy):</p> <table> <thead> <tr> <th>Method</th> <th>AIME25</th> <th>HMMT25</th> </tr> </thead> <tbody> <tr> <td>Full-vocabulary</td> <td><strong>84.1%</strong></td> <td><strong>60.0%</strong></td> </tr> <tr> <td>Sampled-token</td> <td>82.1%</td> <td>57.3%</td> </tr> </tbody> </table> <p>On Qwen3-4B, full-vocabulary logit distillation consistently outperforms the sampled-token objective. The downside is higher memory usage, since vocabulary-sized logits must be stored for many positions.</p> <h2 id="limitations-and-future-directions">Limitations and Future Directions</h2> <p>While OPSD demonstrates strong empirical results up to 8B parameters, several research directions warrant investigation:</p> <p><strong>Scaling to larger models.</strong> The scaling behavior of self-distillation beyond 8B parameters remains an open question. Our hypothesis suggests larger models should exhibit greater benefits, but computational constraints precluded testing at frontier model scales.</p> <p><strong>Verification signal integration.</strong> The current framework does not explicitly incorporate correctness verification. Combining distribution matching with outcome-based verification signals could provide complementary learning objectives.</p> <p><strong>Curriculum learning strategies.</strong> When problems exceed the model’s comprehension threshold, even the teacher policy cannot provide meaningful supervision. Adaptive difficulty adjustment—gradually increasing problem complexity as the model improves—could enhance training effectiveness.</p> <h2 id="conclusion">Conclusion</h2> <p>On-Policy Self-Distillation demonstrates that sufficiently capable models can provide self-supervision by leveraging their ability to rationalize correct solutions. By conditioning a single model on different contexts—with and without privileged information—OPSD achieves:</p> <ul> <li>Superior performance compared to supervised fine-tuning</li> <li>Comparable or better results than reinforcement learning with <strong>4-8× improved token efficiency</strong></li> </ul> <p>The central insight is conceptually elegant yet empirically powerful: evaluation and rationalization are computationally less demanding than generation from scratch. When provided access to correct answers, models can understand the underlying reasoning and effectively guide their weaker counterparts toward improved solutions.</p> <p>As models continue to scale, this self-teaching capability may become increasingly valuable—enabling more efficient training without the overhead of maintaining separate teacher models or the computational burden of extensive reinforcement learning.</p> <hr/> <p><strong>Implementation:</strong> The complete paper includes detailed ablations, implementation specifications, and analysis. All experiments employ the Qwen3 model family with LoRA fine-tuning on 8×A100 GPUs.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[As intelligence scales, learning need not rely solely on external supervision; sufficiently capable systems can refine themselves by reflecting on outcomes. Much like a student reviewing solutions, rationalizing them, and correcting prior mistakes, an LLM can be conditioned on privileged info (e.g., correct solution or a reasoning trace) and supervise its weaker self—the version without such access—by matching the privileged-info-induced distribution from itself.]]></summary></entry></feed>