<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://siyan-zhao.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://siyan-zhao.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-21T09:09:55+00:00</updated><id>https://siyan-zhao.github.io/feed.xml</id><title type="html">Siyan Zhao</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Self-Distilled Reasoner: Teaching LLMs to Teach Themselves</title><link href="https://siyan-zhao.github.io/blog/2026/opsd/" rel="alternate" type="text/html" title="Self-Distilled Reasoner: Teaching LLMs to Teach Themselves"/><published>2026-01-21T00:00:00+00:00</published><updated>2026-01-21T00:00:00+00:00</updated><id>https://siyan-zhao.github.io/blog/2026/opsd</id><content type="html" xml:base="https://siyan-zhao.github.io/blog/2026/opsd/"><![CDATA[<h2 id="the-problem-with-current-methods">The Problem with Current Methods</h2> <p>Large language models have made remarkable progress on complex reasoning tasks, but training them effectively remains challenging. Current approaches each have significant limitations:</p> <p><strong>Supervised Fine-Tuning (SFT)</strong> trains models on fixed expert solutions, but suffers from exposure bias—the model never sees its own mistakes during training, leading to compounding errors at test time.</p> <p><strong>Reinforcement Learning</strong> methods like GRPO address this by training on the model’s own outputs, but they’re computationally expensive and inefficient. They require sampling multiple responses per question (typically 8 or more) and provide only sparse, sequence-level feedback: either the entire answer is right or wrong, with no guidance on which specific reasoning steps went astray.</p> <p><strong>Knowledge Distillation</strong> from a larger teacher model provides dense, token-level supervision, but requires maintaining and running a separate, often larger model—adding substantial computational overhead.</p> <blockquote> <p><strong>INSERT FIGURE 1 HERE: Overview diagram showing the OPSD framework with student and teacher policies</strong></p> </blockquote> <h2 id="key-insight">Key Insight</h2> <p>Our approach is inspired by how humans learn from mistakes. When a student solves a problem incorrectly, they don’t just see a binary “wrong” signal—they can examine the correct solution, understand the reasoning steps, and identify where their thinking diverged.</p> <p>We ask: <strong>Can a language model do the same?</strong> If a model is sufficiently capable, can it rationalize a correct solution when given access to it, and use that understanding to teach its weaker self?</p> <p>The answer is yes. We introduce <strong>On-Policy Self-Distillation (OPSD)</strong>, where a single model plays both roles:</p> <ul> <li><strong>Student policy</strong> \(p_S(\cdot \mid x)\): sees only the problem \(x\), just like at test time</li> <li><strong>Teacher policy</strong> \(p_T(\cdot \mid x, y^*)\): has privileged access to the correct answer \(y^*\)</li> </ul> <p>Both policies share the same parameters—they’re the same model, just conditioned on different contexts.</p> <h2 id="how-opsd-works">How OPSD Works</h2> <p>The training process follows three steps:</p> <p><strong>1. Student generates a solution.</strong> Given a problem \(x\), the student policy samples its own attempt:</p> \[\hat{y} \sim p_S(\cdot \mid x)\] <p><strong>2. Both policies evaluate.</strong> At each token position \(n\), both the student and teacher produce probability distributions over the next token:</p> <ul> <li>Student: \(p_S(\cdot \mid x, \hat{y}_{&lt;n})\)</li> <li>Teacher: \(p_T(\cdot \mid x, y^*, \hat{y}_{&lt;n})\)</li> </ul> <p>The teacher, informed by the correct answer \(y^*\), knows which reasoning paths lead to the solution.</p> <p><strong>3. Minimize the divergence.</strong> We train the student to match the teacher’s distribution at every token:</p> <div class="equation-block"> $$ \mathcal{L}_{\text{OPSD}}(\theta) = \mathbb{E}_{(x,y^*)\sim \mathcal{S}} \mathbb{E}_{\hat{y}\sim p_S(\cdot|x)} \left[ \sum_{n=1}^{|\hat{y}|} D\left(p_T(\cdot \mid x, y^*, \hat{y}_{&lt;n}) \,\|\, p_S(\cdot \mid x, \hat{y}_{&lt;n})\right) \right] $$ </div> <p>Crucially, <strong>gradients only flow through the student’s logits</strong>. The teacher acts as a fixed target, but both are the same model with different conditioning contexts.</p> <blockquote> <p><strong>INSERT FIGURE 2 HERE: Example prompts showing student and teacher conditioning</strong></p> </blockquote> <p>This design combines the best of all worlds:</p> <ul> <li>✓ <strong>On-policy training</strong>: uses the student’s own trajectories</li> <li>✓ <strong>Dense feedback</strong>: token-level guidance at every step</li> <li>✓ <strong>No external teacher</strong>: single model serves both roles</li> <li>✓ <strong>Exploits ground truth</strong>: leverages available correct solutions</li> </ul> <h2 id="results">Results</h2> <p>We evaluate OPSD on competition-level mathematics benchmarks (AIME 2024/2025, HMMT 2025, AMO-Bench) using the Qwen3 model family.</p> <h3 id="performance">Performance</h3> <p>OPSD consistently outperforms both SFT and GRPO across all model sizes:</p> <blockquote> <p><strong>INSERT TABLE 2 HERE: Main results table showing performance across benchmarks</strong></p> </blockquote> <p>The improvements become more pronounced at larger scales. For Qwen3-8B, OPSD achieves:</p> <ul> <li><strong>52.2%</strong> average accuracy vs 51.3% for GRPO and 50.0% for SFT</li> <li>Gains of up to <strong>4.0 points</strong> on HMMT25 over the base model</li> </ul> <h3 id="token-efficiency">Token Efficiency</h3> <p>Perhaps most striking is the computational efficiency. OPSD achieves better performance while using <strong>8× fewer tokens</strong> than GRPO:</p> <blockquote> <p><strong>INSERT FIGURE 3 HERE: Token efficiency comparison showing OPSD vs GRPO</strong></p> </blockquote> <p>This dramatic efficiency gain comes from:</p> <ol> <li>Dense token-level supervision (vs sparse sequence-level rewards)</li> <li>Single rollout per problem (vs 8 rollouts for GRPO)</li> <li>Shorter generation budgets (2k tokens vs 16k for GRPO)</li> </ol> <p>In practice, this translates to <strong>significantly faster training</strong> and <strong>lower computational cost</strong>.</p> <h3 id="scaling-with-multiple-attempts">Scaling with Multiple Attempts</h3> <p>When we allow multiple attempts per problem (pass@k), OPSD’s advantages compound:</p> <blockquote> <p><strong>INSERT FIGURE 4 HERE: Pass@k curves showing performance with different generation lengths</strong></p> </blockquote> <p>Longer generation lengths provide more teacher supervision, with 2k and 4k token budgets substantially outperforming 1k tokens.</p> <h2 id="what-makes-it-work">What Makes It Work?</h2> <p>Through extensive ablations, we uncovered several key factors:</p> <h3 id="model-capacity-matters">Model Capacity Matters</h3> <p>Self-distillation requires sufficient model capability to rationalize solutions. We tested three scales:</p> <blockquote> <p><strong>INSERT FIGURE 5 HERE: Scaling results across 1.7B, 4B, and 8B models</strong></p> </blockquote> <ul> <li><strong>1.7B</strong>: OPSD provides marginal or even negative effects</li> <li><strong>4B</strong>: Consistent improvements emerge</li> <li><strong>8B</strong>: Strongest gains observed</li> </ul> <p>Below a certain capacity threshold, models cannot effectively leverage privileged information—they lack the reasoning ability to understand <em>why</em> the correct answer works.</p> <h3 id="generation-length">Generation Length</h3> <p>Since the objective operates at the token level, longer generations provide more supervision:</p> <div class="highlight-box"> <strong>Key Finding:</strong> Increasing generation length from 1k → 2k → 4k tokens consistently improves performance, with diminishing returns beyond 2k. </div> <p>This makes intuitive sense: more tokens means more opportunities for the teacher to guide the student’s reasoning.</p> <h3 id="full-vocabulary-vs-sampled-token-objectives">Full-Vocabulary vs Sampled-Token Objectives</h3> <p>We compared two ways to compute the divergence:</p> <ol> <li><strong>Full-vocabulary</strong>: Compute \(D(p_T \| p_S)\) over entire vocabulary</li> <li><strong>Sampled-token</strong>: Use only the actually sampled token in a policy-gradient style objective</li> </ol> <p>Results on Qwen3-4B:</p> <table> <thead> <tr> <th>Method</th> <th>AIME25</th> <th>HMMT25</th> </tr> </thead> <tbody> <tr> <td>Full-vocabulary</td> <td><strong>84.1%</strong></td> <td><strong>60.0%</strong></td> </tr> <tr> <td>Sampled-token</td> <td>82.1%</td> <td>57.3%</td> </tr> </tbody> </table> <p>The full-vocabulary approach provides richer supervision by exposing the student to the teacher’s complete distribution over plausible next tokens, though at higher memory cost.</p> <h2 id="limitations-and-future-work">Limitations and Future Work</h2> <p>While OPSD shows strong results up to 8B parameters, several questions remain:</p> <p><strong>Scaling to frontier models.</strong> Does the self-distillation trend continue at 70B+ scale? Our hypothesis suggests larger models should benefit more, but computational constraints prevented us from testing this.</p> <p><strong>Incorporating verification.</strong> OPSD currently doesn’t explicitly use correctness signals. Combining distribution matching with outcome verification could provide additional learning signal.</p> <p><strong>Curriculum learning.</strong> If problems are too difficult, even the teacher policy cannot provide meaningful guidance. Adaptively adjusting problem difficulty as the model improves could enhance training effectiveness.</p> <p><strong>Beyond mathematics.</strong> We focused on mathematical reasoning with verifiable answers. Can OPSD extend to domains with less clear-cut ground truth?</p> <h2 id="conclusion">Conclusion</h2> <p>On-Policy Self-Distillation demonstrates that sufficiently capable models can teach themselves by leveraging their ability to rationalize correct solutions. By conditioning a single model on different contexts—with and without privileged information—OPSD achieves:</p> <ul> <li>Better performance than supervised fine-tuning</li> <li>Comparable or superior results to reinforcement learning</li> <li><strong>8× improved token efficiency</strong></li> <li>No need for external teacher models</li> </ul> <p>The key insight is simple but powerful: evaluation and rationalization are easier than generation from scratch. When a model has access to the answer, it can understand the reasoning and guide its weaker self toward better solutions.</p> <p>As models continue to scale, this self-teaching capability may become increasingly important—enabling more efficient training without the overhead of maintaining separate teacher models or the computational burden of extensive reinforcement learning.</p> <hr/> <p><strong>Code and more details:</strong> The full paper includes additional ablations, implementation details, and analysis. All experiments use the Qwen3 model family with LoRA fine-tuning on 8×A100 GPUs.</p>]]></content><author><name>Siyan Zhao</name></author><summary type="html"><![CDATA[A new framework where a single model acts as both teacher and student, achieving 8× token efficiency over reinforcement learning methods]]></summary></entry></feed>