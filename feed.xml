<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://siyan-zhao.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://siyan-zhao.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-21T20:46:05+00:00</updated><id>https://siyan-zhao.github.io/feed.xml</id><title type="html">Siyan Zhao</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Self-Distilled Reasoner: On-Policy Self-Distillation</title><link href="https://siyan-zhao.github.io/blog/2026/opsd/" rel="alternate" type="text/html" title="Self-Distilled Reasoner: On-Policy Self-Distillation"/><published>2026-01-21T00:00:00+00:00</published><updated>2026-01-21T00:00:00+00:00</updated><id>https://siyan-zhao.github.io/blog/2026/opsd</id><content type="html" xml:base="https://siyan-zhao.github.io/blog/2026/opsd/"><![CDATA[<h2 id="the-challenge-of-current-training-paradigms">The Challenge of Current Training Paradigms</h2> <p>Large language models have shown impressive abilities in complex reasoning tasks, but finding the best ways to train them is still an open problem. Current approaches come with their own trade-offs and limitations:</p> <p><strong>Supervised Fine-Tuning (SFT)</strong> employs fixed expert demonstrations for training but encounters exposure bias—the model’s training distribution diverges from its test-time behavior, as it never observes its own errors during training. This distributional mismatch can lead to compounding errors during autoregressive generation.</p> <p><strong>Reinforcement Learning</strong> methods, such as Group Relative Policy Optimization (GRPO), address this limitation through on-policy training on the model’s own outputs. However, these approaches suffer from computational inefficiency, requiring multiple rollouts per problem instance (typically 8 or more), and provide only sparse, sequence-level feedback signals. The binary nature of outcome verification—correct or incorrect—offers no intermediate guidance regarding which specific reasoning steps are suboptimal.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/opsd_tab1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Comparison of training methods for reasoning tasks. On-Policy Self-Distillation (OPSD) combines the advantages of on-policy training with dense feedback without requiring an external teacher model. </div> <h2 id="core-insight">Core Insight</h2> <p>Our approach draws inspiration from human pedagogical practices. When students solve problems incorrectly, effective learning occurs not through binary feedback alone, but through examination of correct solutions, understanding of reasoning steps, and identification of divergence points in their own reasoning process.</p> <p>We pose the following research question: <strong>Can a sufficiently capable language model leverage its own rationalization abilities for self-improvement?</strong> Specifically, can a model, when provided access to correct solutions, rationalize the reasoning steps and provide supervision to its weaker counterpart (the version without privileged information)?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/opsd_main.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: Overview of On-Policy Self-Distillation framework. A single language model instantiates both student and teacher policies through differential conditioning contexts. </div> <p>We show that the answer is yes through <strong>On-Policy Self-Distillation (OPSD)</strong>, where a single model plays two roles:</p> <ul> <li><strong>Student policy</strong> \(p_S(\cdot \mid x)\): observes only the problem \(x\), replicating inference-time conditions</li> <li><strong>Teacher policy</strong> \(p_T(\cdot \mid x, y^*)\): receives privileged access to the ground-truth solution \(y^*\)</li> </ul> <p>Critically, both policies share identical parameters—they represent the same model under different conditioning contexts.</p> <h2 id="methodology">Methodology</h2> <p>The training procedure consists of three steps:</p> <p><strong>1. On-Policy Sampling from the Student.</strong> For a given problem \(x\), the student policy samples its own attempted solution:</p> \[\hat{y} = (\hat{y}_1,\ldots,\hat{y}_{|\hat{y}|}) \sim p_S(\cdot \mid x)\] <p><strong>2. Teacher-Student Distribution Computation.</strong> Both policies evaluate the student’s generated trajectory \(\hat{y}\). At each token position \(n\), they compute probability distributions over the next token \(y_n \in \mathcal{V}\) conditioned on the same student prefix \(\hat{y}_{&lt;n} = (\hat{y}_1,\ldots,\hat{y}_{n-1})\):</p> \[p_S(y_n \mid x, \hat{y}_{&lt;n}), \qquad p_T(y_n \mid x, y^*, \hat{y}_{&lt;n})\] <p>The teacher policy, informed by the correct solution \(y^*\), provides guidance toward reasoning trajectories that lead to the correct answer.</p> <p><strong>3. Per-Token Distribution Matching.</strong> We instantiate a <strong>full-vocabulary divergence objective</strong> that matches the teacher and student next-token distributions at each position. We define the trajectory-averaged, token-wise divergence:</p> <div class="equation-block"> $$ D(p_T \| p_S)(\hat{y} \mid x) = \frac{1}{|\hat{y}|} \sum_{n=1}^{|\hat{y}|} D\left(p_T(\cdot \mid x, y^*, \hat{y}_{&lt;n}) \,\|\, p_S(\cdot \mid x, \hat{y}_{&lt;n})\right) $$ </div> <p>where \(D\) can be any distribution divergence measure such as the generalized Jensen-Shannon divergence \(\text{JSD}_\beta\), defined for a weight \(\beta \in [0, 1]\) as:</p> \[\text{JSD}_\beta(p_T \| p_S) = \beta D_{\text{KL}}(p_T \| m) + (1 - \beta) D_{\text{KL}}(p_S \| m)\] <p>where \(m = \beta p_T + (1 - \beta) p_S\) is the interpolated mixture distribution. This full-vocabulary formulation provides dense, token-level feedback: the teacher, informed by \(y^*\), exposes the student to the entire distribution over plausible next tokens and guides it toward reasoning paths that lead to the correct answer.</p> <p>We minimize the expected divergence between teacher and student over on-policy student samples:</p> <div class="equation-block"> $$ \mathcal{L}(\theta) = \mathbb{E}_{(x,y^*)\sim \mathcal{S}} \left[ \mathbb{E}_{\hat{y}\sim p_S(\cdot|x)} \left[ D(p_T \| p_S)(\hat{y} \mid x) \right] \right] $$ </div> <p><strong>Gradients flow only through the student’s logits</strong>. The teacher serves as a fixed supervision target, despite both policies sharing the same underlying parameters but differing in their conditioning contexts.</p> <p>Importantly, we fix the teacher policy to be the initial policy, rather than the currently updating learning policy, as we find this helps stabilize training and implicitly acts as regularization to prevent excessive deviation from the initial policy.</p> <p><strong>Alternative Objective: Sampled-Token Distillation.</strong> As an alternative approach, we can use a policy-gradient formulation that operates only on sampled tokens. For each position \(n\), we define the advantage term:</p> \[A_n(x, \hat{y}) = \log p_T(\hat{y}_n \mid x, y^*, \hat{y}_{&lt;n}) - \log p_S(\hat{y}_n \mid x, \hat{y}_{&lt;n})\] <p>and optimize:</p> \[\mathcal{L}(\theta) = - \mathbb{E}_{(x,y^*) \sim \mathcal{S}} \left[ \mathbb{E}_{\hat{y} \sim p_S(\cdot \mid x)} \left[ \frac{1}{|\hat{y}|} \sum_{n=1}^{|\hat{y}|} A_n(x, \hat{y}) \log p_S(\hat{y}_n \mid x, \hat{y}_{&lt;n}) \right] \right]\] <p>where \(A_n(x,\hat{y})\) is treated as a constant (gradients do not flow through the advantage). Compared to the full-vocabulary divergence objective, this sampled-token approach uses the teacher’s log-probabilities to provide dense trajectory-level shaping signals without explicitly matching the full distribution at each step.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/prompt_exampel_opsd.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3: Prompt construction for student and teacher policies. Both policies operate with identical parameters but receive different conditioning information. </div> <h2 id="experimental-results">Experimental Results</h2> <p>We evaluate OPSD on competition-level mathematical reasoning benchmarks (AIME 2024/2025, HMMT 2025, AMO-Bench) using the Qwen3 model family (1.7B, 4B, and 8B parameters).</p> <h3 id="performance-comparison">Performance Comparison</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/main_result.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4: Main results comparing OPSD against baseline methods across benchmarks. </div> <h3 id="computational-efficiency">Computational Efficiency</h3> <p>A particularly notable finding concerns computational efficiency. OPSD achieves superior performance while utilizing <strong>8× fewer tokens</strong> than GRPO:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/opsd_tokenefficiency.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5: Token efficiency comparison between OPSD and GRPO. OPSD achieves higher performance with substantially reduced token consumption. </div> <p>This efficiency advantage stems from:</p> <ol> <li>Dense token-level supervision (versus sparse sequence-level rewards)</li> <li>Reduced generation budgets (2k tokens versus 16k for GRPO)</li> </ol> <p>In practice, this translates to <strong>reduced training time</strong> and <strong>lower computational requirements</strong>.</p> <h2 id="ablation-studies">Ablation Studies</h2> <h3 id="model-capacity-requirements">Model Capacity Requirements</h3> <p>Self-distillation requires sufficient model capacity to effectively rationalize solutions. We conducted experiments across three model scales:</p> <div class="highlight-box"> <strong>Key Finding:</strong> A minimum capacity threshold exists for effective self-distillation. </div> <ul> <li><strong>1.7B parameters</strong>: OPSD yields marginal or negative effects</li> <li><strong>4B parameters</strong>: Consistent improvements emerge</li> <li><strong>8B parameters</strong>: Most substantial gains observed</li> </ul> <p>Below the capacity threshold, models lack the requisite reasoning ability to understand and rationalize why correct solutions work, even when provided access to them.</p> <h3 id="generation-length-analysis">Generation Length Analysis</h3> <p>Since the training objective operates at the token level, generation length directly impacts the quantity of supervision signal available:</p> <p>Increasing generation length from 1k → 2k → 4k tokens yields consistent performance improvements. This finding aligns with the intuitive understanding that more tokens provide additional opportunities for the teacher policy to guide the student’s reasoning process.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opsd/gen_length_ablation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 6: Pass@k performance curves showing the effect of generation length. Longer generation budgets provide increased teacher supervision. </div> <h3 id="objective-function-comparison">Objective Function Comparison</h3> <p>We compared two approaches for computing the divergence:</p> <ol> <li><strong>Full-vocabulary divergence</strong>: Compute \(D(p_T \| p_S)\) over the entire vocabulary</li> <li><strong>Sampled-token objective</strong>: Utilize only the sampled token in a policy-gradient formulation</li> </ol> <p>Results on Qwen3-4B:</p> <table> <thead> <tr> <th>Method</th> <th>AIME25</th> <th>HMMT25</th> </tr> </thead> <tbody> <tr> <td>Full-vocabulary</td> <td><strong>84.1%</strong></td> <td><strong>60.0%</strong></td> </tr> <tr> <td>Sampled-token</td> <td>82.1%</td> <td>57.3%</td> </tr> </tbody> </table> <p>The full-vocabulary approach provides richer supervision by exposing the student to the teacher’s complete distribution over plausible next tokens, though at increased memory cost.</p> <h2 id="limitations-and-future-directions">Limitations and Future Directions</h2> <p>While OPSD demonstrates strong empirical results up to 8B parameters, several research directions warrant investigation:</p> <p><strong>Scaling to larger models.</strong> The scaling behavior of self-distillation beyond 8B parameters remains an open question. Our hypothesis suggests larger models should exhibit greater benefits, but computational constraints precluded testing at frontier model scales.</p> <p><strong>Verification signal integration.</strong> The current framework does not explicitly incorporate correctness verification. Combining distribution matching with outcome-based verification signals could provide complementary learning objectives.</p> <p><strong>Curriculum learning strategies.</strong> When problems exceed the model’s comprehension threshold, even the teacher policy cannot provide meaningful supervision. Adaptive difficulty adjustment—gradually increasing problem complexity as the model improves—could enhance training effectiveness.</p> <h2 id="conclusion">Conclusion</h2> <p>On-Policy Self-Distillation demonstrates that sufficiently capable models can provide self-supervision by leveraging their ability to rationalize correct solutions. By conditioning a single model on different contexts—with and without privileged information—OPSD achieves:</p> <ul> <li>Superior performance compared to supervised fine-tuning</li> <li>Comparable or better results than reinforcement learning with <strong>8× improved token efficiency</strong></li> </ul> <p>The central insight is conceptually elegant yet empirically powerful: evaluation and rationalization are computationally less demanding than generation from scratch. When provided access to correct answers, models can understand the underlying reasoning and effectively guide their weaker counterparts toward improved solutions.</p> <p>As models continue to scale, this self-teaching capability may become increasingly valuable—enabling more efficient training without the overhead of maintaining separate teacher models or the computational burden of extensive reinforcement learning.</p> <hr/> <p><strong>Additional resources:</strong> The complete paper includes detailed ablations, implementation specifications, and analysis. All experiments employ the Qwen3 model family with LoRA fine-tuning on 8×A100 GPUs. Code and additional materials are available in the full publication.</p>]]></content><author><name>Siyan Zhao (Project Lead)</name></author><summary type="html"><![CDATA[As models continue to scale, the self-teaching capability may become increasingly valuable—enabling more efficient training. Much like a student reviewing solutions, rationalizing them, and correcting their prior mistakes, an LLM can be conditioned on the correct solution/reasoning trace and supervise its weaker self (the version without access to the solution) by matching the privileged-info induced distribution from itself.]]></summary></entry></feed>